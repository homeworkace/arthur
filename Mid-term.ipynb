{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f1ca22-d997-4628-a24d-79f772774a29",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Downloading dependencies\n",
    "###### *Cell 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b7e7f-4d3b-4e26-ad2a-ddbfa10c18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# If it ain't here, you pip it. https://www.w3schools.com/python/python_ref_modules.asp\n",
    "!pip install --upgrade kaggle\n",
    "!pip install --upgrade pandas\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e86789-cb72-4d7a-a4ed-79103c7e203f",
   "metadata": {},
   "source": [
    "## Importing dependencies\n",
    "###### *Cell 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a71e04-703d-4e03-8a0e-325bc3364b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb712b3-285c-401a-8ac5-e44138a7acbc",
   "metadata": {},
   "source": [
    "## Initialising the Kaggle CLI\n",
    "### Option #1: New token\n",
    "If you don't already have a token or have lost the file to your current token, in the settings of your Kaggle account, click on the button 'Generate New Token'. Follow the instructions and copy the alphanumeric string at the top when such a floating dialogue appears. Then, run cell 3 and paste the key when prompted.\n",
    "###### *Cell 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deada122-bb1f-4e51-bf91-9d43afe44c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_key() :\n",
    "    key = input('Paste your token here, or leave empty if your token comes in the form of a JSON file: ')\n",
    "    clear_output()\n",
    "    if len(key) > 3 :\n",
    "        # Create api.txt in working directory, where `KAGGLE_API_TOKEN {key}`.\n",
    "        api_file = Path(\"api.txt\")\n",
    "        with api_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write('KAGGLE_API_TOKEN ' + key)\n",
    "        os.environ['KAGGLE_API_TOKEN'] = key\n",
    "        os.environ.pop('KAGGLE_USERNAME', None)\n",
    "        os.environ.pop('KAGGLE_KEY', None)\n",
    "        print('The last four characters of your API key are: ' + key[-4:] + '. If you suspect that you have entered something wrong, run cell 4 again. Otherwise, you may move to the next section.')\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "def legacy_kaggle_key() :\n",
    "    '''\n",
    "    Opens a file dialog, validates the selection, and returns the path \n",
    "    if a valid JSON file is selected. Handles all edge cases.\n",
    "    '''\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "    \n",
    "    file_path = tk.filedialog.askopenfilename(\n",
    "        title='Find the file with your Kaggle API key...',\n",
    "        filetypes=(('JSON files', '*.json'), ('All files', '*.*'))\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "\n",
    "    if not file_path :\n",
    "        # Case: User closes the dialog without selecting anything\n",
    "        print('No file selected. Run cell 5 again if you like.')\n",
    "        return\n",
    "    \n",
    "    # Now try to open and validate the *contents* of the JSON file\n",
    "    try :\n",
    "        with open(file_path, 'r') as f :\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Case: User selects the correct JSON file that contains their API key\n",
    "        if 'key' in data and isinstance(data['username'], str) and isinstance(data['key'], str) :\n",
    "            # Create api.txt in working directory, where `KAGGLE_USERNAME {data['username']}` and `KAGGLE_KEY {data['key']}`.\n",
    "            api_file = Path(\"api.txt\")\n",
    "            with api_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                f.write('KAGGLE_USERNAME ' + data['username'] + '\\nKAGGLE_KEY ' + data['key'])\n",
    "            os.environ['KAGGLE_USERNAME'] = data['username']\n",
    "            os.environ['KAGGLE_KEY'] = data['key']\n",
    "            os.environ.pop('KAGGLE_API_TOKEN', None)\n",
    "            print('The last four characters of your API key are: ' + data['key'][-4:] + '. If you suspect that this is not an alphanumeric string, find another file by running cell 5 again. Otherwise, you may move to the next section.')\n",
    "            \n",
    "        else :\n",
    "            # Case: User selects a JSON file, but it's not one that contains their API key\n",
    "            print('The \\'key\\' field is missing or invalid. To find another file, run this cell again.')\n",
    "            \n",
    "    except Exception :\n",
    "        print('This file may not contain valid JSON. To find another file, run this cell again.')\n",
    "        return\n",
    "\n",
    "# If api.txt is in the working directory, separate by newline, then separate by spaces. Each line has the form '{key} {value}`, where the environment variable `key` should be created with value `value`.\n",
    "# Otherwise, run `kaggle_key()`. If that returns `False`, then run `legacy_kaggle_key()`.\n",
    "\n",
    "api_file = Path('api.txt')\n",
    "\n",
    "if api_file.exists() :\n",
    "    try :\n",
    "        with api_file.open('r', encoding='utf-8') as f :\n",
    "            for line in f :\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#') :\n",
    "                    continue\n",
    "\n",
    "                parts = line.split(None, 1)  # split on first whitespace\n",
    "                if len(parts) != 2 :\n",
    "                    continue\n",
    "\n",
    "                env_key, env_value = parts\n",
    "                os.environ[env_key] = env_value\n",
    "\n",
    "    except OSError as e :\n",
    "        print('Found api.txt but could not read it:\\n' + e)\n",
    "else :\n",
    "    print('api.txt was not found.')\n",
    "\n",
    "preview = os.environ.get('KAGGLE_API_TOKEN') or os.environ.get('KAGGLE_KEY')\n",
    "if preview:\n",
    "    print('Loaded existing credentials from api.txt. The last four characters of your API key are: ' + preview[-4:] + '. If that looks wrong, run cell 4 or 5.')\n",
    "else:\n",
    "    # No api.txt present; go through interactive flow\n",
    "    if not kaggle_key():\n",
    "        legacy_kaggle_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e321ddf-464d-485b-8b0d-4fc0dfa13f67",
   "metadata": {},
   "source": [
    "###### *Cell 4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150af2dc-317e-4ba2-a108-6d71cebe585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not kaggle_key() :\n",
    "    print('You probably didn\\'t enter a valid key. Run this cell again if you like.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772c154-cd34-4e30-9b1e-1b79de6dd68c",
   "metadata": {},
   "source": [
    "###### *Cell 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7704b2f-945b-467b-988e-fd67a1993838",
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_kaggle_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10834cbc-f972-45cf-a1c0-f02db6bc8c0f",
   "metadata": {},
   "source": [
    "###### *Cell 6*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd855225-7788-4eae-8e37-4ca68427328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_module_description = !pip show kaggle\n",
    "os.environ['PATH'] = os.environ['PATH'] + kaggle_module_description[-3][10:-13] + 'Scripts;'\n",
    "!kaggle datasets download flkuhm/art-price-dataset -p dataset -f artDataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c37655-6e06-48be-887b-299f803bcdae",
   "metadata": {},
   "source": [
    "I assume:\n",
    "- Your kernel is running on Python 3.13, and Windows 11.\n",
    "- You have 'tcl/tk and IDLE' checked this Python environment was installed. In other words, if you were to create and run a cell anywhere in this notebook with the following line `!pip freeze`, you are able to find `tkinter` in the output.\n",
    "- You are not running the kernel with any virtual environment.\n",
    "# Preparing the dataset\n",
    "###### *Cell 7*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50220f9f-195f-4e5d-a20a-8b0e6e6f2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = dataset = pd.read_csv('dataset/artDataset.csv')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f2cdf-03c7-4029-8c47-6356a8709337",
   "metadata": {},
   "source": [
    "The current columns are:\n",
    "- `Unnamed: 0`: why even lol\n",
    "- `price`: Numerical\n",
    "- `artist`: Categorical\n",
    "- `title`: Not needed?\n",
    "- `yearCreation`: Numerical, but can be split into a categorical component\n",
    "- `signed`: Word frequency\n",
    "- `condition`: Word frequency\n",
    "- `period`: Categorical, but might line up with `yearCreation`.\n",
    "- `movement`: Categorical or word frequency\n",
    "\n",
    "## `Unnamed: 0`\n",
    "At first glance, the values in this column line up with the values as prescribed by the leftmost index column. There are a variety of parameters when using `pandas.DataFrame.to_csv()` to save a pandas DataFrame to `.csv`. If there is at least one column filled entirely with unique, non-empty values, `index_label` can be used to designate one of them as the index column. Otherwise, `index` can be used to influence whether a new column of indices is created. If an index-like column already exists in the DataFrame, but isn't designated as such, pandas will treat it like any other column, as it could contain real information. As the `Unnamed: 0` column seems to be a common enough phenomenon within datasets uploaded to Kaggle (https://www.kaggle.com/discussions/general/354943), I believe this is what happened in the creation of this dataset.\n",
    "\n",
    "To find out if `Unnamed: 0` is effectively an index column, I created a filter to find any rows whose `Unnamed: 0` value is different from the index column's.\n",
    "###### Cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381f5ae-c55e-4b98-8e79-67c82e3fd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['Unnamed: 0'] != dataset.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2091c893-8bd7-4d5b-83b6-ac02d80c24ba",
   "metadata": {},
   "source": [
    "There were none. Therefore, all rows had values which corresponded with their indices. Since `Unnamed: 0` is an index column and does not contain any other unique information, I chose to remove it.\n",
    "###### Cell 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b222e2-b250-4363-816e-4eeb04159f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9b08c-45cb-42d6-8d85-e2086a0fc99a",
   "metadata": {},
   "source": [
    "## `price`\n",
    "First, I establish that these values are being saved as strings, and that for sorting to occur, I must convert them into a numerical representation. I observe that the prices are annotated with \"USD\". I show that all rows have prices in USD, which precludes any currency conversion. Then, as part of the conversion to a numerical representation, I remove \" USD\" from all of these values.\n",
    "\n",
    "Second, I assume two things: that the period is only used as a thousands separator, like in the continental system, and that all prices are natural numbers. I show that there are no other characters in all the strings than `[0-9.]+`. Then, I show that all rows fit into one of two patterns: no period, or a period succeeded by 3 digits. Finally, I acknowledge the edge case where prices are shown to 3 decimal places. However, I believe this is highly unlikely as the US dollar is denominated down to a cent, which is a hundredth of a dollar, and invite the reader to manually review the dataset in case of further doubt. I proceed to remove the periods from all the values, and parse them all as numbers.\n",
    "\n",
    "In order to demonstrate that the period is only used as a thousands separator, after removing the currency tag, I show that all entires fit into one of two patterns: no period, or a period succeeded by 3 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc8100-a1b9-46df-8107-424bcec2e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90530585-2e39-48e2-b057-118eb8cdd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_usd = raw_dataset['price'].str.contains('USD')\n",
    "raw_dataset[in_usd]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c1667-5357-4b25-a6e3-984deacb8c37",
   "metadata": {},
   "source": [
    "Of course, within the second group, there could be entries whose prices are shown to three decimal places (i.e., thousandths of a US dollar). However, I believe this is highly unlikely as the US dollar is denominated down to a cent, which is a hundredth of a dollar. In case of any further doubts, you may manually review the dataset on Kaggle or a spreadsheet viewer of your choice. You may find and remove any entry whose price you feel should be reasonably interpreted as being a thousandth of a dollar, and run this section again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9aa216-86c6-40bd-8d9d-976ad2a8b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the \" USD\" text from the end of the strings\n",
    "# The regex=False makes it slightly faster for a fixed string replacement\n",
    "cleaned_prices = raw_dataset['price'].str.replace(' USD', '', regex=False)\n",
    "cleaned_prices = cleaned_prices.str.replace('.', '', regex=False)\n",
    "\n",
    "# 2. Convert the resulting clean numeric strings to a float data type\n",
    "# pd.to_numeric is generally robust and efficient\n",
    "raw_dataset['price_numeric'] = pd.to_numeric(cleaned_prices)\n",
    "\n",
    "# Optional: You can drop the old string column\n",
    "#raw_dataset = raw_dataset.drop(columns=['price'])\n",
    "\n",
    "# Check the results\n",
    "#print(raw_dataset.head())\n",
    "#print(raw_dataset.info())\n",
    "#raw_dataset['price_numeric']\n",
    "raw_dataset.sort_values(by=['price_numeric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ee542-c118-4404-b6d8-7543b4059969",
   "metadata": {},
   "source": [
    "## `title`\n",
    "I acknowledge that the title of an artwork influences a prospective buyer. First impressions matter in a marketplace of many works, and an engaging title would help a particular piece stand out to them, rather than be glossed over. However, most titles are unique to a single work and there is no obvious ordinal or numerical structure. To use title as an input to a linear regression model would require transforming the text into a set of bag-of-words features, which, due to the small sample size and large word space, would result in high sparsity and greatly increase the risk of overfitting. Therefore, I decided to retain `title` for my reference, but exclude it from the set of predictive features.\n",
    "\n",
    "I created a filter to exclude these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a517f-60cb-4de4-b537-9183b2e115a5",
   "metadata": {},
   "source": [
    "## `yearCreation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e80cc8-3595-406b-890b-4da1f8a0587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_conversion = dataset['yearCreation'].value_counts(dropna = False).to_frame()\n",
    "year_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ce427-51ca-4255-8cd0-dbe691e67362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_numerical_year(year) :\n",
    "    if year.isnumeric() :\n",
    "        return int(year) + 0.5\n",
    "    return None\n",
    "\n",
    "def rule_numerical_year_interval(year) :\n",
    "    if year.isnumeric() :\n",
    "        return 1\n",
    "    return None\n",
    "\n",
    "year_conversion['year'] = year_conversion.index.map(rule_numerical_year)\n",
    "year_conversion['year_interval'] = year_conversion.index.map(rule_numerical_year_interval)\n",
    "\n",
    "unconverted_strings = year_conversion['year'].isna()\n",
    "year_conversion[~unconverted_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63345e7-7f2c-4e08-9d16-f4c4c730e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_circa_year(year) :\n",
    "    if not year.startswith('Circa '):\n",
    "        return None\n",
    "    year = year[6:]\n",
    "    if year.isnumeric():\n",
    "        return int(year)\n",
    "    return None\n",
    "\n",
    "def rule_circa_year_interval(year) :\n",
    "    if not year.startswith('Circa '):\n",
    "        return None\n",
    "    year = year[6:]\n",
    "    if year.isnumeric():\n",
    "        return 5\n",
    "    return None\n",
    "\n",
    "year_conversion.loc[unconverted_strings, 'year'] = year_conversion.loc[unconverted_strings].index.map(rule_circa_year)\n",
    "year_conversion.loc[unconverted_strings, 'year_interval'] = year_conversion.loc[unconverted_strings].index.map(rule_circa_year_interval)\n",
    "\n",
    "unconverted_strings = year_conversion['year'].isna()\n",
    "year_conversion[unconverted_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1901ef-7d41-467d-916e-35d2f7eb855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_year_range(year) :\n",
    "    if year.startswith('Circa '):\n",
    "        year = year[6:]\n",
    "    start_end = year.replace(' ', '').split('-')\n",
    "    if not len(start_end) == 2 :\n",
    "        return None\n",
    "    if not (start_end[0].isnumeric() and start_end[1].isnumeric()) :\n",
    "        return None\n",
    "    return (int(start_end[0]) + int(start_end[1])) / 2 + 0.5\n",
    "\n",
    "def rule_year_range_interval(year) :\n",
    "    if year.startswith('Circa '):\n",
    "        year = year[6:]\n",
    "    start_end = year.replace(' ', '').split('-')\n",
    "    if not len(start_end) == 2 :\n",
    "        return None\n",
    "    if not (start_end[0].isnumeric() and start_end[1].isnumeric()) :\n",
    "        return None\n",
    "    return int(start_end[1]) - int(start_end[0]) + 1\n",
    "\n",
    "year_conversion.loc[unconverted_strings, 'year'] = year_conversion.loc[unconverted_strings].index.map(rule_year_range)\n",
    "year_conversion.loc[unconverted_strings, 'year_interval'] = year_conversion.loc[unconverted_strings].index.map(rule_year_range_interval)\n",
    "\n",
    "unconverted_strings = year_conversion['year'].isna()\n",
    "year_conversion[unconverted_strings].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57595a-0c76-40dd-9a7c-1e387099583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['yearCreation'] == '1998 / 2011']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1d959-9972-409d-9dbd-dd5d1a939812",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_conversion.loc['Second Half 20th Century ', ['year', 'year_interval']] = [1975, 50]\n",
    "year_conversion.loc['21st Century ', ['year', 'year_interval']] = [2011.11, 22.22]\n",
    "year_conversion.loc['2022', ['year', 'year_interval']] = [2022.11, 0.22] # By this logic, \"2022\" should also only span from January the 1st to March the 20th.\n",
    "year_conversion.loc['Late 20th Century ', ['year', 'year_interval']] = [1983 + 1 / 3, 100 / 3]\n",
    "year_conversion.loc['Mid 20th Century ', ['year', 'year_interval']] = [1950, 100 / 3]\n",
    "year_conversion.loc['Late 19th Century ', ['year', 'year_interval']] = [1883 + 1 / 3, 100 / 3]\n",
    "year_conversion.loc['Early 20th Century ', ['year', 'year_interval']] = [1916 + 2 / 3, 100 / 3]\n",
    "year_conversion.loc['First Half 20th Century ', ['year', 'year_interval']] = [1925, 50]\n",
    "year_conversion.loc['19th Century ', ['year', 'year_interval']] = [1850, 100]\n",
    "year_conversion.loc['20th Century ', ['year', 'year_interval']] = [1950, 100]\n",
    "year_conversion.loc['Second Half 19th Century ', ['year', 'year_interval']] = [1875, 50]\n",
    "year_conversion.loc['1961, printed in 2010', ['year', 'year_interval']] = [2010.5, 1]\n",
    "year_conversion.loc[year_conversion.index.str.contains('3D'), ['year', 'year_interval']] = [2019.5, 1]\n",
    "year_conversion.loc['1998 / 2011', ['year', 'year_interval']] = [2011.5, 1]\n",
    "year_conversion.loc['Printed 1984', ['year', 'year_interval']] = [1984.5, 1]\n",
    "year_conversion.loc['[nan]', ['year', 'year_interval']] = [1911.11, 222.22] # January the 1st, 1800, to Match the 20th, 2022.\n",
    "\n",
    "year_conversion[year_conversion['year'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08a12d-e092-4aab-8773-f2c93a5fa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.insert(4, 'year', dataset['yearCreation'].map(year_conversion['year']))\n",
    "dataset.insert(5, 'year_interval', dataset['yearCreation'].map(year_conversion['year_interval']))\n",
    "dataset = dataset.drop(columns = ['yearCreation'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2f481-65ec-4100-92e4-6e1e23e61859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['period'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2032cd-50ff-4153-8585-f7e7b465cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['period'] == '[nan]') & (dataset['year'] != 1911.11)].sort_values(['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4a137-699d-42ca-b796-da61b8b97420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['period'] == '19th Century') & (dataset['year'] != 1911.11)].sort_values(['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cd0bc-89fc-4d05-9f31-afe186b37af9",
   "metadata": {},
   "source": [
    "# Visualisations\n",
    "Mean mode median? Spread skewness\n",
    "## Correlation between `period` and `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bdc5d8-c01c-440b-b3cf-3f4a20edb0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
