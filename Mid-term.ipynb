{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f1ca22-d997-4628-a24d-79f772774a29",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Downloading dependencies\n",
    "###### *Cell 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b7e7f-4d3b-4e26-ad2a-ddbfa10c18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# If it ain't here, you pip it. https://www.w3schools.com/python/python_ref_modules.asp\n",
    "!pip install --upgrade kaggle\n",
    "!pip install --upgrade pandas\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e86789-cb72-4d7a-a4ed-79103c7e203f",
   "metadata": {},
   "source": [
    "## Importing dependencies\n",
    "###### *Cell 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a71e04-703d-4e03-8a0e-325bc3364b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb712b3-285c-401a-8ac5-e44138a7acbc",
   "metadata": {},
   "source": [
    "## Initialising the Kaggle CLI\n",
    "### Option #1: New token\n",
    "If you don't already have a token or have lost the file to your current token, in the settings of your Kaggle account, click on the button 'Generate New Token'. Follow the instructions and copy the alphanumeric string at the top when such a floating dialogue appears. Then, run cell 3 and paste the key when prompted.\n",
    "###### *Cell 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deada122-bb1f-4e51-bf91-9d43afe44c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_key() :\n",
    "    key = input('Paste your token here, or leave empty if your token comes in the form of a JSON file: ')\n",
    "    clear_output()\n",
    "    if len(key) > 3 :\n",
    "        # Create api.txt in working directory, where `KAGGLE_API_TOKEN {key}`.\n",
    "        api_file = Path(\"api.txt\")\n",
    "        with api_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write('KAGGLE_API_TOKEN ' + key)\n",
    "        os.environ['KAGGLE_API_TOKEN'] = key\n",
    "        os.environ.pop('KAGGLE_USERNAME', None)\n",
    "        os.environ.pop('KAGGLE_KEY', None)\n",
    "        print('The last four characters of your API key are: ' + key[-4:] + '. If you suspect that you have entered something wrong, run cell 4 again. Otherwise, you may move to the next section.')\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "def legacy_kaggle_key() :\n",
    "    '''\n",
    "    Opens a file dialog, validates the selection, and returns the path \n",
    "    if a valid JSON file is selected. Handles all edge cases.\n",
    "    '''\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "    \n",
    "    file_path = tk.filedialog.askopenfilename(\n",
    "        title='Find the file with your Kaggle API key...',\n",
    "        filetypes=(('JSON files', '*.json'), ('All files', '*.*'))\n",
    "    )\n",
    "    \n",
    "    root.destroy()\n",
    "\n",
    "    if not file_path :\n",
    "        # Case: User closes the dialog without selecting anything\n",
    "        print('No file selected. Run cell 5 again if you like.')\n",
    "        return\n",
    "    \n",
    "    # Now try to open and validate the *contents* of the JSON file\n",
    "    try :\n",
    "        with open(file_path, 'r') as f :\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Case: User selects the correct JSON file that contains their API key\n",
    "        if 'key' in data and isinstance(data['username'], str) and isinstance(data['key'], str) :\n",
    "            # Create api.txt in working directory, where `KAGGLE_USERNAME {data['username']}` and `KAGGLE_KEY {data['key']}`.\n",
    "            api_file = Path(\"api.txt\")\n",
    "            with api_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                f.write('KAGGLE_USERNAME ' + data['username'] + '\\nKAGGLE_KEY ' + data['key'])\n",
    "            os.environ['KAGGLE_USERNAME'] = data['username']\n",
    "            os.environ['KAGGLE_KEY'] = data['key']\n",
    "            os.environ.pop('KAGGLE_API_TOKEN', None)\n",
    "            print('The last four characters of your API key are: ' + data['key'][-4:] + '. If you suspect that this is not an alphanumeric string, find another file by running cell 5 again. Otherwise, you may move to the next section.')\n",
    "            \n",
    "        else :\n",
    "            # Case: User selects a JSON file, but it's not one that contains their API key\n",
    "            print('The \\'key\\' field is missing or invalid. To find another file, run this cell again.')\n",
    "            \n",
    "    except Exception :\n",
    "        print('This file may not contain valid JSON. To find another file, run this cell again.')\n",
    "        return\n",
    "\n",
    "# If api.txt is in the working directory, separate by newline, then separate by spaces. Each line has the form '{key} {value}`, where the environment variable `key` should be created with value `value`.\n",
    "# Otherwise, run `kaggle_key()`. If that returns `False`, then run `legacy_kaggle_key()`.\n",
    "\n",
    "api_file = Path('api.txt')\n",
    "\n",
    "if api_file.exists() :\n",
    "    try :\n",
    "        with api_file.open('r', encoding='utf-8') as f :\n",
    "            for line in f :\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#') :\n",
    "                    continue\n",
    "\n",
    "                parts = line.split(None, 1)  # split on first whitespace\n",
    "                if len(parts) != 2 :\n",
    "                    continue\n",
    "\n",
    "                env_key, env_value = parts\n",
    "                os.environ[env_key] = env_value\n",
    "\n",
    "    except OSError as e :\n",
    "        print('Found api.txt but could not read it:\\n' + e)\n",
    "else :\n",
    "    print('api.txt was not found.')\n",
    "\n",
    "preview = os.environ.get('KAGGLE_API_TOKEN') or os.environ.get('KAGGLE_KEY')\n",
    "if preview:\n",
    "    print('Loaded existing credentials from api.txt. The last four characters of your API key are: ' + preview[-4:] + '. If that looks wrong, run cell 4 or 5.')\n",
    "else:\n",
    "    # No api.txt present; go through interactive flow\n",
    "    if not kaggle_key():\n",
    "        legacy_kaggle_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e321ddf-464d-485b-8b0d-4fc0dfa13f67",
   "metadata": {},
   "source": [
    "###### *Cell 4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150af2dc-317e-4ba2-a108-6d71cebe585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not kaggle_key() :\n",
    "    print('You probably didn\\'t enter a valid key. Run this cell again if you like.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772c154-cd34-4e30-9b1e-1b79de6dd68c",
   "metadata": {},
   "source": [
    "###### *Cell 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7704b2f-945b-467b-988e-fd67a1993838",
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_kaggle_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10834cbc-f972-45cf-a1c0-f02db6bc8c0f",
   "metadata": {},
   "source": [
    "###### *Cell 6*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd855225-7788-4eae-8e37-4ca68427328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_module_description = !pip show kaggle\n",
    "os.environ['PATH'] = os.environ['PATH'] + kaggle_module_description[-3][10:-13] + 'Scripts;'\n",
    "!kaggle datasets download flkuhm/art-price-dataset -p dataset -f artDataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c37655-6e06-48be-887b-299f803bcdae",
   "metadata": {},
   "source": [
    "I assume:\n",
    "- Your kernel is running on Python 3.13, and Windows 11.\n",
    "- You have 'tcl/tk and IDLE' checked this Python environment was installed. In other words, if you were to create and run a cell anywhere in this notebook with the following line `!pip freeze`, you are able to find `tkinter` in the output.\n",
    "- You are not running the kernel with any virtual environment.\n",
    "# Preparing the dataset\n",
    "###### *Cell 7*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50220f9f-195f-4e5d-a20a-8b0e6e6f2961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_dataset = dataset = pd.read_csv('dataset/artDataset.csv')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f2cdf-03c7-4029-8c47-6356a8709337",
   "metadata": {},
   "source": [
    "The current columns are:\n",
    "- `Unnamed: 0`: why even lol\n",
    "- `price`: Numerical\n",
    "- `artist`: Categorical\n",
    "- `title`: Not needed?\n",
    "- `yearCreation`: Numerical, but can be split into a categorical component\n",
    "- `signed`: Word frequency\n",
    "- `condition`: Word frequency\n",
    "- `period`: Categorical, but might line up with `yearCreation`.\n",
    "- `movement`: Categorical or word frequency\n",
    "\n",
    "## `Unnamed: 0`\n",
    "At first glance, the values in this column line up with the values as prescribed by the leftmost index column. There are a variety of parameters when using `pandas.DataFrame.to_csv()` to save a pandas DataFrame to `.csv`. If there is at least one column filled entirely with unique, non-empty values, `index_label` can be used to designate one of them as the index column. Otherwise, `index` can be used to influence whether a new column of indices is created. If an index-like column already exists in the DataFrame, but isn't designated as such, pandas will treat it like any other column, as it could contain real information. As the `Unnamed: 0` column seems to be a common enough phenomenon within datasets uploaded to Kaggle (https://www.kaggle.com/discussions/general/354943), I believe this is what happened in the creation of this dataset.\n",
    "\n",
    "To find out if `Unnamed: 0` is effectively an index column, I created a filter to find any rows whose `Unnamed: 0` value is different from the index column's.\n",
    "###### *Cell 8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381f5ae-c55e-4b98-8e79-67c82e3fd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['Unnamed: 0'] != dataset.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2091c893-8bd7-4d5b-83b6-ac02d80c24ba",
   "metadata": {},
   "source": [
    "There were none. Therefore, all rows had values which corresponded with their indices. Since `Unnamed: 0` is an index column and does not contain any other unique information, I chose to remove it.\n",
    "###### *Cell 9*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b222e2-b250-4363-816e-4eeb04159f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9b08c-45cb-42d6-8d85-e2086a0fc99a",
   "metadata": {},
   "source": [
    "## `price`\n",
    "The prices in each row seem to follow a pattern. The United States dollar (USD) is specified as a currency, and the thousands are separated by periods (`.`). This is reminiscent of a convention of writing large numbers that is popular in continental Europe. In contrast to the Anglophone convention, which uses commas (`,`) to separate thousands, and a single period to mark the beginning of the decimal part, the continental convention swaps them around. Nevertheless, characters like these, including the `USD` suffix, cause the prices to be represented as strings. I will need to convert them into numerical representations so that I can carry out sorting and linear regression.\n",
    "\n",
    "I sought to verify these assumptions:\n",
    "1. That all prices are denominated in USD.\n",
    "2. That the period is used as a thousands separator at all times.\n",
    "\n",
    "First, I searched for any rows whose price was not annnotated with `' USD'`.\n",
    "###### *Cell 10*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51468085-9d88-44f2-9542-2a145f696e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[~dataset['price'].str.endswith(' USD')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335967b6-b71a-4528-ab73-fa7b2095ab73",
   "metadata": {},
   "source": [
    "There were none, meaning that all prices were annotated in `' USD'`. There could be additional clerical errors, like `'USD'` appearing more than once. To test that, I provisionally removed the `' USD'` suffix in a copy of the column, then tested for the presence of additional letters by combining all the strings into one and listing all the unique characters within that string.\n",
    "###### *Cell 11*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3353d83-f93a-444a-8153-7236e99106b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_price_numerical = dataset['price'].str.replace(' USD', '')\n",
    "set(column_price_numerical.str.cat())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf4a7b-a69b-4b28-904f-58c3a5b2f2e6",
   "metadata": {},
   "source": [
    "The removal of `' USD'` was a success, and caused all the strings to no longer contain any letters. This proved to me that all prices are indeed denominated in USD, removing the need to perform currency conversions. Furthermore, the period is the only symbol to appear, with whitespaces (` `) and commas notably absent.\n",
    "\n",
    "I could use this uniformity in formatting to my advantage, and further assume that all prices are natural numbers, i.e., none of the prices have decimal parts. I still considered the edge case that the period is used both as a thousands separator and a decimal separator, and to disprove that I checked the prices against the following regular expression `^\\d{1,3}(\\.\\d{3})*$`. Here are some characteristics:\n",
    "- Without a period, there can only be up to 3 free digits. With periods, there can only be up to 3 leading digits before the leftmost period.\n",
    "- There must be at least 1 free digit. Without a period, this ensures the number has at least 1 digit. With periods, this ensures the leftmost period is not exposed (`0.100` as opposed to `.100`).\n",
    "- Every period must be succeeded by exactly 3 digits. There can be as many groups as possible of periods and 3 trailing digits to represent the powers of a thousand, like millions and billions.\n",
    "- This expression specifically disallows the common convention of denoting cents with a period and 2 trailing digits.\n",
    "\n",
    "###### *Cell 12*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc8100-a1b9-46df-8107-424bcec2e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_price_numerical.str.fullmatch(r'^\\d{1,3}(\\.\\d{3})*$').all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc35fd8-7d5b-4d94-ab2b-baa9522b810b",
   "metadata": {},
   "source": [
    "Since all prices fit the above pattern, I'm sufficiently convinced that all prices are to the nearest dollar, and that the period is only ever used as a thousands separator. I acknowledge the deeper edge case that prices are shown to three decimal places (i.e., thousandths of a US dollar). However, apart from being highly unlikely that prices are denominated in anything other than dollars and cents, the smallest division of the US dollar is the cent, which is a hundredth of a dollar. Should the agreed-upon price contain part of a cent, buyers may find it hard to pay the exact amount. Still, in case of any further doubt, you may manually review the dataset on Kaggle or a spreadsheet viewer of your choice. You may find and remove any entry whose price you feel should be reasonably interpreted as being a thousandth of a dollar, and run this section again.\n",
    "\n",
    "Since I assumed the period is not a decimal separator, I assumed too that removing periods will not cause the price to be interpreted as 100 times larger. Thus, I removed all the periods and parsed all strings as integers, which completed the preprocessing of this column. The prices could now be sorted and used in regression, as seen below.\n",
    "###### *Cell 13*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90530585-2e39-48e2-b057-118eb8cdd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['price'] = pd.to_numeric(column_price_numerical.str.replace('.', ''))\n",
    "dataset.sort_values(by = ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c1667-5357-4b25-a6e3-984deacb8c37",
   "metadata": {},
   "source": [
    "## `artist`\n",
    "Some artists are more famous than others, and consistently command higher prices. Therefore, the provenance of a work should be a predictor of its price. To model this, I sought to turn this column into two features:\n",
    "  - Dummy (one-hot) encodings for prolific artists. The result is as many columns of dummy encodings as there are artists I wish to track. Any work would take a value of `1` on the column that represents its artist, and `0` everywhere else. The names for each of these columns shall be copied from the strings in `artist`.\n",
    "  - `works_by_artist`: The number of works in this dataset that are from the same artist. I hypothesised that even if a buyer was unfamiliar with a particular artist, the fact that an auction house's catalogue features many works from the same artist could give an impression of relevance, popularity or market confidence. The result is a numerical column.\n",
    "\n",
    "My main consideration was the variability introduced during data entry. Different works may have been catalogued decades apart, by different appraisers, who may or may not follow the prevailing formatting guides of their time. Auction houses somewhat mitigate this through authority control, using internal databases to reconcile pseudonyms, name changes (e.g., after marriage), and other variants of a creator’s identity. However, mismatches still arise, and many must be resolved by the curators'/buyers' understanding. For example, if a system doesn't have the right heuristics to link two identities together, it may consider two works, attributed to \"PICASSO\" and \"Pablo Picasso\" respectively, to be unrelated. It is up to the buyer to make that connection between the two names, and correctly identify that either of these works are just as valuable as the other.\n",
    "\n",
    "Considering that this dataset may contain legacy formatting and clerical inconsistencies, and considering that a heuristic approach would not fit within the amount of effort I expect in this section, I decided on human judgement as the most practical choice in consolidating artist names. So, I performed a manual pass to merge entries that were clearly intended to refer to the same person. First, I needed to grasp the space of possible data I needed to process.\n",
    "###### *Cell 14*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6dace-2475-4e29-b20a-c3a42ad3aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tablify(array, columns, transpose = False) :\n",
    "    rows = int(np.ceil(len(array) / columns))\n",
    "    array_copy = array + [''] * (rows * columns - len(array))\n",
    "\n",
    "    if transpose :\n",
    "        array_as_table = np.array(array_copy).reshape(columns, rows).transpose()\n",
    "    else :\n",
    "        array_as_table = np.array(array_copy).reshape(rows, columns)\n",
    "    print(pd.DataFrame(array_as_table).to_string(index = False, header = False))\n",
    "\n",
    "artist_names = dataset['artist'].value_counts(dropna = False).sort_index().index.to_list()\n",
    "tablify(artist_names, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e5496-7bd7-4fed-b2dc-c0a98a4d9be3",
   "metadata": {},
   "source": [
    "I created a helper function that displays long lists in a tabular format, and passed in the unique artist names alphabetically. This sped up my discovery of potential clerical inconsistencies. In many cases, small formatting variations such as differences in spacing, punctuation, or capitalisation become obvious when the entries are viewed side-by-side. For example, I could immediately see on the rightmost column that \"T.L. Solien\" appeared just below \"T. L. Solien\", as well as another variant a little further up: \"Solien T.L.\".\n",
    "\n",
    "Listed last in the alphabetical ordering, was `nan`.\n",
    "###### *Cell 15*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48939b70-907c-4276-bf01-c8674cbba748",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['artist'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31af11-2aa6-4909-b4e2-371c87821d31",
   "metadata": {},
   "source": [
    "There was surprisingly only one `nan` value in this entire dataset. However, this lone `nan` caused friction with many of the pandas functions, and I had to include extra parameters in my function calls, like `dropna = False`. So I replaced it with the string \"unknown\". This choice avoided conflicts with real artist names, and retained its convenient position at the bottom of alphabetical listings.\n",
    "###### *Cell 16*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea748d8b-697e-49f9-82a3-1b405dda4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[725, 'artist'] = 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b4bfd-bf58-42b1-8612-a80f1f6e6641",
   "metadata": {},
   "source": [
    "With the full set of unique names in alphabetical order, I began manually identifying connections between these names—an example of data linkage, as I later learnt the process is called.\n",
    "\n",
    "ChatGPT was useful in accelerating the discovery of possible matches. As someone unfamiliar with all of the artists in the dataset, it brought to the table contextual information, such as an artist's history, aliases, or typical subject matter. With this knowledge, it could propose matches that I wouldn't have otherwise considered. However, given the length of the list of names, it hallucinated often, made up new names and proposed links between unrelated artists. Therefore, I used these suggestions as prompts for further verification rather than as authoritative claims.\n",
    "###### *Cell 17*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64497c3-a7c9-4759-bcf7-60ba53283c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['artist'].str.contains('Kota')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b390459-e873-4e39-9303-8bd99822dff5",
   "metadata": {},
   "source": [
    "One such case where I overruled ChatGPT was with [Kota Ezawa](https://en.wikipedia.org/wiki/Kota_Ezawa). He is a German and Japanese artist currently based in San Francisco, whose works remix existing media (like photographs and movie stills) in his style of flat colours and a limited palette. As a result, the titles of his works contain credits to the source material, in the form of \"(After)\".\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://sothebys-md.brightspotcdn.com/72/8a/8076c8744baeafc979cf7d160c85/bsp7f-front.png\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://sothebys-md.brightspotcdn.com/e3/21/b2d2428f4169b020afa671e01d5f/bsp7g-front.png\" />\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\"> <!-- Use colspan to span across 2 columns -->\n",
    "            <p><center><i>\"Conical Intersect\" (left) and \"The Bohemians\" (right). Images provided by Sotheby's online catalogue.</i></center></p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "ChatGPT suggested that \"The Bohemians (After August Sander 1924)\", attributed to \"After Kota Ezawa\", was the work of an unnamed artist, who was reinterpreting Ezawa’s own interpretation of August Sander. However, I considered this unlikely. If The Bohemians had been made by someone else, I would expect that it was presented in a different condition than the other genuine Ezawas, be offered at a different price, or was at least dated later than them. But in all these respects it was similar to the other Ezawas. Most of the descriptive fields had a consistent format, if not outright identical, and appeared in adjacent index positions. This implied to me that they were catalogued at the same time and by the same appraiser.\n",
    "\n",
    "Still, I verified my assumptions with a manual search. Although the work has been made unavailable today, I found its [listing](https://www.sothebys.com/en/buy/_the-bohemians-after-august-sander-1924-from-the-history-of-photography-remix-3eed). Here, I saw that not only were the style and colour palette similar to the other Ezawas, the description, which is missing from the dataset as a field, also read:\n",
    "> Kota Ezawa (German, b. 1969).\n",
    ">\n",
    "> This piece is final sale and not eligible for return.\n",
    "\n",
    "With this, I concluded that ChatGPT was wrong in this instance, and that The Bohemians was indeed created by Kota Ezawa.\n",
    "\n",
    "After identifying variants of a single artist's name, the next step was to make one of them the *canonical form*: a single identity with which all other variants shall be reconciled. All works in the dataset that had been attributed to any of these variants were re-attributed to this canonical form. Typically, I chose the identity with the most amount of works to be the canonical form.\n",
    "###### *Cell 18*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cac2a-03d3-48d2-a2e8-83c4f5dcd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge variants of \"T.L. Solien\"\n",
    "dataset['artist'] = dataset['artist'].replace(['T. L. Solien', 'Solien T.L.'], 'T.L. Solien')\n",
    "\n",
    "# Merge variants of \"Kota Ezawa\"\n",
    "dataset['artist'] = dataset['artist'].replace('After Kota Ezawa', 'Kota Ezawa')\n",
    "\n",
    "# Merge variants of \"John Andre Gundelfinger\"\n",
    "dataset['artist'] = dataset['artist'].replace('John Gundelfinger', 'John Andre Gundelfinger')\n",
    "\n",
    "# Merge variants of \"Richard Woods\"\n",
    "dataset['artist'] = dataset['artist'].replace('RICHARD WOODS', 'Richard Woods')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284499e-8821-4365-8652-8078195108e9",
   "metadata": {},
   "source": [
    "As I do not have the expertise of an art curator, I was necessarily conservative in discovering links between the various identities. Beyond the clerical variants, I did not make further assumptions about any other pair of names, and it is possible that some pseudonyms still lie undiscovered.\n",
    "\n",
    "Now that the works have been (sufficiently) correctly attributed, some artists now have more works to their name, which more accurately reflects their perceived popularity in the online gallery. I computed this number for each artist and added them to each of their works as a numerical feature: `works_by_artist`, as I had introduced in the beginning of this section.\n",
    "\n",
    "Notably, I assigned a value of 0 to the work with the unknown artist. While it is certain that this work was created by someone, the absence of a traceable or recognisable identity removes any potential authorship premium. From a buyer’s perspective, unknown authorship introduces uncertainty and eliminates the possibility of prior recognition, institutional validation, or reputational signalling. Even artists represented by a single work retain a non-zero probability of name recognition, whereas an unknown artist does not. For this reason, I treat the unknown artist as having zero name-based recognition. Any value attributed to this work must therefore arise from other observable characteristics, such as aesthetic qualities, condition, or alignment with prevailing trends.\n",
    "###### *Cell 19*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd75d12-fb9a-47d9-8bcf-b0eb79fc990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_popularity = dataset['artist'].value_counts()\n",
    "dataset.insert(loc = 2, column = 'works_by_artist', value = dataset['artist'].map(artist_popularity))\n",
    "dataset.loc[725, 'works_by_artist'] = 0\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621dcd9-be90-4b89-9b22-89a76f331b5b",
   "metadata": {},
   "source": [
    "# Let's take a break for the sponsor of this notebook:\n",
    "![](https://attachments-cdn-s.coub.com/coub_storage/coub/simple/cw_image/6f66b8f55da/2a6042a632c0744d1e9fe/1448960136_00030.jpg)  \n",
    "While `works_by_artist` sought to capture the effect of confidence derived from seeing many works from the same artist, individual artists carry their own reputations too. Just knowing that a work was created by a renowned artist can add to its perceived value. I sought to encode this aforementioned \"authorship premium\" by adding a column for each artist, denoting whether a work was attributed to them. This allows a model to learn the \"value\" of an artist's name, independent of the qualities and aesthetics of their works.\n",
    "\n",
    "However, as it was seen in the preview of the dataset above, and as seen below, the majority of artists appear only once: 318 out of 449.\n",
    "###### *Cell 20*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e3724-8046-42bc-9590-2de227864118",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_popularity = artist_popularity.drop('unknown')\n",
    "artist_attribution_thresholds = artist_popularity.value_counts().sort_index(ascending = False)\n",
    "artist_attribution_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09124c8d-d972-491c-b95e-4df31e94f065",
   "metadata": {},
   "source": [
    "Dummy variables are extremely sparse; a column corresponding to an artist with a single work would take the value 1 for exactly one row and 0 everywhere else. As such a feature carries no repeatable signal, its coefficient would be estimated from a single observation and would therefore primarily capture noise rather than a stable effect attributable to the artist’s reputation. Including hundreds of such sparse features would greatly increase the dimensionality of the dataset without providing meaningful predictive power, increasing the risk of overfitting. At the same time, I consider artist indicators to act as tags rather than numerical features. I'm not measuring the *Pavel Tchelitchew*-ness of a work just like I am the price or year of creation. Instead, if a work had been made by Pavel Tchelitchew, the value associated with the tag is added to its predicted price, like a bias. Under this reasoning, one-work artists account for 318 of the 754 works (42%), and excluding these tags from being learned would remove any possibility of modelling name-based effects for a large subset of works.\n",
    "\n",
    "In the end, I decided to track every single artist (apart from \"unknown\") in a separate DataFrame.\n",
    "###### *Cell 21*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43795f4-024a-4f32-91ed-18755067c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_attributions = pd.DataFrame(\n",
    "    {\n",
    "        artist: (dataset['artist'] == artist).astype(int)\n",
    "        for artist in artist_popularity.to_dict()\n",
    "    },\n",
    "    index=dataset.index\n",
    ")\n",
    "artist_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68b6c2-c99a-4f32-b61e-d1cfb02edfac",
   "metadata": {},
   "source": [
    "This completes the first set of features that I had introduced in the beginning of the section: dummy encodings for prolific artists. `artist_attributions` is designed to be modular; the columns are ordered in descending artist representation, and leftmost columns represent artists who have the most works. Rather than permanently committing to a fixed subset of artist indicators, a selected subset of its columns will be appended to the dataset. `artist_attribution_thresholds`, as shown in cell 20, is a reference for how many of the columns to include for training. It shows that the first 3 columns represent artists who have 13 or more works. The next 19 columns represented artists who have 5 or more works, and so on. By progressively including more columns and therefore less represented artists, I hoped to find the balance between the comprehensive tracking of artists' values and the noisiness of the resulting artist tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ee542-c118-4404-b6d8-7543b4059969",
   "metadata": {},
   "source": [
    "## `title`\n",
    "I acknowledge that the title of an artwork influences a prospective buyer. First impressions matter in a marketplace of many works, and an engaging title would help a particular piece stand out to them, rather than be glossed over. However, most titles are unique to a single work and there is no obvious ordinal or numerical structure. To use title as an input to a linear regression model would require transforming the text into a set of bag-of-words features, which, due to the small sample size and large word space, would result in high sparsity and greatly increase the risk of overfitting. Therefore, I decided to retain `title` for my reference, but exclude it from the set of predictive features.\n",
    "\n",
    "I created a filter to exclude these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a517f-60cb-4de4-b537-9183b2e115a5",
   "metadata": {},
   "source": [
    "## `yearCreation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e80cc8-3595-406b-890b-4da1f8a0587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_conversion = dataset['yearCreation'].value_counts(dropna = False).to_frame()\n",
    "year_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ce427-51ca-4255-8cd0-dbe691e67362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_numerical_year(year) :\n",
    "    if year.isnumeric() :\n",
    "        return int(year) + 0.5\n",
    "    return None\n",
    "\n",
    "def rule_numerical_year_interval(year) :\n",
    "    if year.isnumeric() :\n",
    "        return 1\n",
    "    return None\n",
    "\n",
    "year_conversion['year'] = year_conversion.index.map(rule_numerical_year)\n",
    "year_conversion['year_interval'] = year_conversion.index.map(rule_numerical_year_interval)\n",
    "\n",
    "unconverted_strings = year_conversion['year'].isna()\n",
    "year_conversion[~unconverted_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63345e7-7f2c-4e08-9d16-f4c4c730e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_circa_year(year) :\n",
    "    if not year.startswith('Circa '):\n",
    "        return None\n",
    "    year = year[6:]\n",
    "    if year.isnumeric():\n",
    "        return int(year)\n",
    "    return None\n",
    "\n",
    "def rule_circa_year_interval(year) :\n",
    "    if not year.startswith('Circa '):\n",
    "        return None\n",
    "    year = year[6:]\n",
    "    if year.isnumeric():\n",
    "        return 5\n",
    "    return None\n",
    "\n",
    "year_conversion.loc[unconverted_strings, 'year'] = year_conversion.loc[unconverted_strings].index.map(rule_circa_year)\n",
    "year_conversion.loc[unconverted_strings, 'year_interval'] = year_conversion.loc[unconverted_strings].index.map(rule_circa_year_interval)\n",
    "\n",
    "unconverted_strings = year_conversion['year'].isna()\n",
    "year_conversion[unconverted_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1901ef-7d41-467d-916e-35d2f7eb855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_year_range(year) :\n",
    "    if year.startswith('Circa '):\n",
    "        year = year[6:]\n",
    "    start_end = year.replace(' ', '').split('-')\n",
    "    if not len(start_end) == 2 :\n",
    "        return None\n",
    "    if not (start_end[0].isnumeric() and start_end[1].isnumeric()) :\n",
    "        return None\n",
    "    return (int(start_end[0]) + int(start_end[1])) / 2 + 0.5\n",
    "\n",
    "def rule_year_range_interval(year) :\n",
    "    if year.startswith('Circa '):\n",
    "        year = year[6:]\n",
    "    start_end = year.replace(' ', '').split('-')\n",
    "    if not len(start_end) == 2 :\n",
    "        return None\n",
    "    if not (start_end[0].isnumeric() and start_end[1].isnumeric()) :\n",
    "        return None\n",
    "    return int(start_end[1]) - int(start_end[0]) + 1\n",
    "\n",
    "year_conversion.loc[unconverted_strings, 'year'] = year_conversion.loc[unconverted_strings].index.map(rule_year_range)\n",
    "year_conversion.loc[unconverted_strings, 'year_interval'] = year_conversion.loc[unconverted_strings].index.map(rule_year_range_interval)\n",
    "\n",
    "unconverted_strings = year_conversion['year'].isna()\n",
    "year_conversion[unconverted_strings].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57595a-0c76-40dd-9a7c-1e387099583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['yearCreation'] == '1998 / 2011']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1d959-9972-409d-9dbd-dd5d1a939812",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_conversion.loc['Second Half 20th Century ', ['year', 'year_interval']] = [1975, 50]\n",
    "year_conversion.loc['21st Century ', ['year', 'year_interval']] = [2011.11, 22.22]\n",
    "year_conversion.loc['2022', ['year', 'year_interval']] = [2022.11, 0.22] # By this logic, \"2022\" should also only span from January the 1st to March the 20th.\n",
    "year_conversion.loc['Late 20th Century ', ['year', 'year_interval']] = [1983 + 1 / 3, 100 / 3]\n",
    "year_conversion.loc['Mid 20th Century ', ['year', 'year_interval']] = [1950, 100 / 3]\n",
    "year_conversion.loc['Late 19th Century ', ['year', 'year_interval']] = [1883 + 1 / 3, 100 / 3]\n",
    "year_conversion.loc['Early 20th Century ', ['year', 'year_interval']] = [1916 + 2 / 3, 100 / 3]\n",
    "year_conversion.loc['First Half 20th Century ', ['year', 'year_interval']] = [1925, 50]\n",
    "year_conversion.loc['19th Century ', ['year', 'year_interval']] = [1850, 100]\n",
    "year_conversion.loc['20th Century ', ['year', 'year_interval']] = [1950, 100]\n",
    "year_conversion.loc['Second Half 19th Century ', ['year', 'year_interval']] = [1875, 50]\n",
    "year_conversion.loc['1961, printed in 2010', ['year', 'year_interval']] = [2010.5, 1]\n",
    "year_conversion.loc[year_conversion.index.str.contains('3D'), ['year', 'year_interval']] = [2019.5, 1]\n",
    "year_conversion.loc['1998 / 2011', ['year', 'year_interval']] = [2011.5, 1]\n",
    "year_conversion.loc['Printed 1984', ['year', 'year_interval']] = [1984.5, 1]\n",
    "year_conversion.loc['[nan]', ['year', 'year_interval']] = [1911.11, 222.22] # January the 1st, 1800, to Match the 20th, 2022.\n",
    "\n",
    "year_conversion[year_conversion['year'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08a12d-e092-4aab-8773-f2c93a5fa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.insert(4, 'year', dataset['yearCreation'].map(year_conversion['year']))\n",
    "dataset.insert(5, 'year_interval', dataset['yearCreation'].map(year_conversion['year_interval']))\n",
    "dataset = dataset.drop(columns = ['yearCreation'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2f481-65ec-4100-92e4-6e1e23e61859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['period'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2032cd-50ff-4153-8585-f7e7b465cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['period'] == '[nan]') & (dataset['year'] != 1911.11)].sort_values(['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4a137-699d-42ca-b796-da61b8b97420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[(dataset['period'] == '19th Century') & (dataset['year'] != 1911.11)].sort_values(['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cd0bc-89fc-4d05-9f31-afe186b37af9",
   "metadata": {},
   "source": [
    "# Visualisations\n",
    "Mean mode median? Spread skewness\n",
    "## Correlation between `period` and `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bdc5d8-c01c-440b-b3cf-3f4a20edb0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
